{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'details', 'categories', 'url'])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sumin/opt/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=None, categories=None, drop=None,\n",
       "              dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
       "              n_values=None, sparse=True)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(y[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = enc.transform(y[:,np.newaxis]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], Y[:60000], Y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000, 10) (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "X_val = X_train[50000:]\n",
    "y_val = y_train[50000:]\n",
    "X_train = X_train[:50000]\n",
    "y_train = y_train[:50000]\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, W):\n",
    "    K = np.size(W, 1)\n",
    "    A = np.exp(X @ W)\n",
    "    B = np.diag(1 / (np.reshape(A @ np.ones((K,1)), -1)))\n",
    "    Y = B @ A\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 정규화\n",
    "아래의 구현에서 처럼 기존에 구해놓은 cost값에서 regularization을 위한 값을 빼주면 새로운 코스트를 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, T, W, lambda_):\n",
    "    epsilon = 1e-5\n",
    "    N = len(T)\n",
    "    K = np.size(T, 1)\n",
    "    cost = - (1/N) * np.ones((1,N)) @ (np.multiply(np.log(softmax(X, W) + epsilon), T)) @ np.ones((K,1))\n",
    "    cost -= (lambda_ / (N * 2.)) * np.sum(np.square(W))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W):\n",
    "    return np.argmax((X @ W), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gd(X, T, W, learning_rate, iterations, lambda_ ,batch_size):\n",
    "    N = len(T)\n",
    "    cost_history = np.zeros((iterations,1))\n",
    "    shuffled_indices = np.random.permutation(N)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    T_shuffled = T[shuffled_indices]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        j = i % N\n",
    "        X_batch = X_shuffled[j:j+batch_size]\n",
    "        T_batch = T_shuffled[j:j+batch_size]\n",
    "        # batch가 epoch 경계를 넘어가는 경우, 앞 부분으로 채워줌\n",
    "        if X_batch.shape[0] < batch_size:\n",
    "            X_batch = np.vstack((X_batch, X_shuffled[:(batch_size - X_batch.shape[0])]))\n",
    "            T_batch = np.vstack((T_batch, T_shuffled[:(batch_size - T_batch.shape[0])]))\n",
    "        W = W - (learning_rate/batch_size) * (X_batch.T @ (softmax(X_batch, W) - T_batch))\n",
    "        cost_history[i] = compute_cost(X_batch, T_batch, W, lambda_)\n",
    "        #if i % 1000 == 0:\n",
    "        #    print(cost_history[i][0])\n",
    "\n",
    "    return (cost_history, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda구하기\n",
    "입력 데이터를 10000장으로 줄인 X_val을 입력값으로 하고 iteration을 10000으로 조정하여 lambda값을 구한다. lambda의 값을 0.005씩 늘려주면서 가장 높은 점수를 기록하는 lambda값을 찾고 이 값을 통해 모델을 트레이닝 시킨후 스코어를 구하면, 기존의 값보다 높아졌음을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.8967, lambda:0\n",
      "score: 0.8944, lambda:0.005\n",
      "score: 0.8935, lambda:0.01\n",
      "score: 0.8807, lambda:0.015\n",
      "score: 0.8954, lambda:0.02\n",
      "score: 0.8926, lambda:0.025\n",
      "score: 0.892, lambda:0.030000000000000002\n",
      "score: 0.8965, lambda:0.035\n",
      "score: 0.891, lambda:0.04\n",
      "score: 0.8964, lambda:0.045\n",
      "score: 0.8932, lambda:0.049999999999999996\n",
      "score: 0.8949, lambda:0.05499999999999999\n",
      "score: 0.888, lambda:0.05999999999999999\n",
      "score: 0.8974, lambda:0.06499999999999999\n",
      "score: 0.8937, lambda:0.06999999999999999\n",
      "score: 0.8956, lambda:0.075\n",
      "score: 0.8934, lambda:0.08\n",
      "score: 0.8842, lambda:0.085\n",
      "score: 0.8973, lambda:0.09000000000000001\n",
      "score: 0.9, lambda:0.09500000000000001\n",
      "score: 0.8953, lambda:0.10000000000000002\n",
      "score: 0.8935, lambda:0.10500000000000002\n",
      "score: 0.8923, lambda:0.11000000000000003\n",
      "score: 0.8957, lambda:0.11500000000000003\n",
      "score: 0.8953, lambda:0.12000000000000004\n",
      "score: 0.8945, lambda:0.12500000000000003\n",
      "score: 0.8964, lambda:0.13000000000000003\n",
      "score: 0.8898, lambda:0.13500000000000004\n",
      "score: 0.8954, lambda:0.14000000000000004\n",
      "score: 0.8967, lambda:0.14500000000000005\n",
      "score: 0.897, lambda:0.15000000000000005\n",
      "score: 0.8904, lambda:0.15500000000000005\n",
      "score: 0.8953, lambda:0.16000000000000006\n",
      "score: 0.8985, lambda:0.16500000000000006\n",
      "score: 0.8914, lambda:0.17000000000000007\n",
      "score: 0.8975, lambda:0.17500000000000007\n",
      "score: 0.8954, lambda:0.18000000000000008\n",
      "score: 0.8913, lambda:0.18500000000000008\n",
      "score: 0.8948, lambda:0.19000000000000009\n",
      "score: 0.8982, lambda:0.1950000000000001\n",
      "score: 0.8904, lambda:0.2000000000000001\n",
      "score: 0.8913, lambda:0.2050000000000001\n",
      "score: 0.8921, lambda:0.2100000000000001\n",
      "score: 0.8934, lambda:0.2150000000000001\n",
      "score: 0.8987, lambda:0.2200000000000001\n",
      "score: 0.8906, lambda:0.22500000000000012\n",
      "score: 0.8966, lambda:0.23000000000000012\n",
      "score: 0.8938, lambda:0.23500000000000013\n",
      "score: 0.887, lambda:0.24000000000000013\n",
      "score: 0.894, lambda:0.24500000000000013\n",
      "score: 0.8925, lambda:0.2500000000000001\n",
      "score: 0.8918, lambda:0.2550000000000001\n",
      "score: 0.8969, lambda:0.2600000000000001\n",
      "score: 0.8946, lambda:0.2650000000000001\n",
      "score: 0.8924, lambda:0.27000000000000013\n",
      "score: 0.8917, lambda:0.27500000000000013\n",
      "score: 0.899, lambda:0.28000000000000014\n",
      "score: 0.8919, lambda:0.28500000000000014\n",
      "score: 0.8972, lambda:0.29000000000000015\n",
      "score: 0.8966, lambda:0.29500000000000015\n",
      "score: 0.893, lambda:0.30000000000000016\n",
      "score: 0.8949, lambda:0.30500000000000016\n",
      "score: 0.8974, lambda:0.31000000000000016\n",
      "score: 0.8872, lambda:0.31500000000000017\n",
      "score: 0.8988, lambda:0.3200000000000002\n",
      "score: 0.8918, lambda:0.3250000000000002\n",
      "score: 0.8903, lambda:0.3300000000000002\n",
      "score: 0.8925, lambda:0.3350000000000002\n",
      "score: 0.8982, lambda:0.3400000000000002\n",
      "score: 0.8949, lambda:0.3450000000000002\n",
      "score: 0.8966, lambda:0.3500000000000002\n",
      "score: 0.8907, lambda:0.3550000000000002\n",
      "score: 0.8896, lambda:0.3600000000000002\n",
      "score: 0.8952, lambda:0.3650000000000002\n",
      "score: 0.8995, lambda:0.3700000000000002\n",
      "score: 0.896, lambda:0.3750000000000002\n",
      "score: 0.8908, lambda:0.3800000000000002\n",
      "score: 0.8949, lambda:0.38500000000000023\n",
      "score: 0.8953, lambda:0.39000000000000024\n",
      "score: 0.8951, lambda:0.39500000000000024\n",
      "score: 0.886, lambda:0.40000000000000024\n",
      "score: 0.8968, lambda:0.40500000000000025\n",
      "score: 0.8969, lambda:0.41000000000000025\n",
      "score: 0.8961, lambda:0.41500000000000026\n",
      "score: 0.8978, lambda:0.42000000000000026\n",
      "score: 0.8976, lambda:0.42500000000000027\n",
      "score: 0.8948, lambda:0.43000000000000027\n",
      "score: 0.8959, lambda:0.4350000000000003\n",
      "score: 0.8965, lambda:0.4400000000000003\n",
      "score: 0.8987, lambda:0.4450000000000003\n",
      "score: 0.8948, lambda:0.4500000000000003\n",
      "score: 0.8956, lambda:0.4550000000000003\n",
      "score: 0.8922, lambda:0.4600000000000003\n",
      "score: 0.8997, lambda:0.4650000000000003\n",
      "score: 0.8936, lambda:0.4700000000000003\n",
      "score: 0.8969, lambda:0.4750000000000003\n",
      "score: 0.899, lambda:0.4800000000000003\n",
      "score: 0.8895, lambda:0.4850000000000003\n",
      "score: 0.8986, lambda:0.4900000000000003\n",
      "score: 0.8969, lambda:0.49500000000000033\n",
      "score: 0.8979, lambda:0.5000000000000003\n",
      "score: 0.8896, lambda:0.5050000000000003\n",
      "score: 0.8895, lambda:0.5100000000000003\n",
      "score: 0.8934, lambda:0.5150000000000003\n",
      "score: 0.8978, lambda:0.5200000000000004\n",
      "score: 0.8947, lambda:0.5250000000000004\n",
      "score: 0.8916, lambda:0.5300000000000004\n",
      "score: 0.8966, lambda:0.5350000000000004\n",
      "score: 0.8962, lambda:0.5400000000000004\n",
      "score: 0.8927, lambda:0.5450000000000004\n",
      "score: 0.8935, lambda:0.5500000000000004\n",
      "score: 0.8906, lambda:0.5550000000000004\n",
      "score: 0.8918, lambda:0.5600000000000004\n",
      "score: 0.8927, lambda:0.5650000000000004\n",
      "score: 0.8962, lambda:0.5700000000000004\n",
      "score: 0.8913, lambda:0.5750000000000004\n",
      "score: 0.8953, lambda:0.5800000000000004\n",
      "score: 0.8976, lambda:0.5850000000000004\n",
      "score: 0.8988, lambda:0.5900000000000004\n",
      "score: 0.8938, lambda:0.5950000000000004\n",
      "score: 0.8964, lambda:0.6000000000000004\n",
      "score: 0.8957, lambda:0.6050000000000004\n",
      "score: 0.8919, lambda:0.6100000000000004\n",
      "score: 0.894, lambda:0.6150000000000004\n",
      "score: 0.8981, lambda:0.6200000000000004\n",
      "score: 0.8934, lambda:0.6250000000000004\n",
      "score: 0.8856, lambda:0.6300000000000004\n",
      "score: 0.8953, lambda:0.6350000000000005\n",
      "score: 0.893, lambda:0.6400000000000005\n",
      "score: 0.8943, lambda:0.6450000000000005\n",
      "score: 0.8913, lambda:0.6500000000000005\n",
      "score: 0.8951, lambda:0.6550000000000005\n",
      "score: 0.8978, lambda:0.6600000000000005\n",
      "score: 0.8945, lambda:0.6650000000000005\n",
      "score: 0.8974, lambda:0.6700000000000005\n",
      "score: 0.893, lambda:0.6750000000000005\n",
      "score: 0.8962, lambda:0.6800000000000005\n",
      "score: 0.8967, lambda:0.6850000000000005\n",
      "score: 0.8924, lambda:0.6900000000000005\n",
      "score: 0.8936, lambda:0.6950000000000005\n",
      "score: 0.8934, lambda:0.7000000000000005\n",
      "score: 0.8933, lambda:0.7050000000000005\n",
      "score: 0.8887, lambda:0.7100000000000005\n",
      "score: 0.898, lambda:0.7150000000000005\n",
      "score: 0.8921, lambda:0.7200000000000005\n",
      "score: 0.8933, lambda:0.7250000000000005\n",
      "score: 0.896, lambda:0.7300000000000005\n",
      "score: 0.8951, lambda:0.7350000000000005\n",
      "score: 0.8956, lambda:0.7400000000000005\n",
      "score: 0.89, lambda:0.7450000000000006\n",
      "score: 0.8998, lambda:0.7500000000000006\n",
      "score: 0.8975, lambda:0.7550000000000006\n",
      "score: 0.8949, lambda:0.7600000000000006\n",
      "score: 0.8948, lambda:0.7650000000000006\n",
      "score: 0.8947, lambda:0.7700000000000006\n",
      "score: 0.8895, lambda:0.7750000000000006\n",
      "score: 0.8949, lambda:0.7800000000000006\n",
      "score: 0.893, lambda:0.7850000000000006\n",
      "score: 0.8914, lambda:0.7900000000000006\n",
      "score: 0.898, lambda:0.7950000000000006\n",
      "score: 0.8966, lambda:0.8000000000000006\n",
      "score: 0.8969, lambda:0.8050000000000006\n",
      "score: 0.8917, lambda:0.8100000000000006\n",
      "score: 0.892, lambda:0.8150000000000006\n",
      "score: 0.8934, lambda:0.8200000000000006\n",
      "score: 0.8933, lambda:0.8250000000000006\n",
      "score: 0.8896, lambda:0.8300000000000006\n",
      "score: 0.8921, lambda:0.8350000000000006\n",
      "score: 0.8946, lambda:0.8400000000000006\n",
      "score: 0.8949, lambda:0.8450000000000006\n",
      "score: 0.8986, lambda:0.8500000000000006\n",
      "score: 0.8958, lambda:0.8550000000000006\n",
      "score: 0.8976, lambda:0.8600000000000007\n",
      "score: 0.8915, lambda:0.8650000000000007\n",
      "score: 0.8906, lambda:0.8700000000000007\n",
      "score: 0.894, lambda:0.8750000000000007\n",
      "score: 0.8959, lambda:0.8800000000000007\n",
      "score: 0.8913, lambda:0.8850000000000007\n",
      "score: 0.8852, lambda:0.8900000000000007\n",
      "score: 0.8993, lambda:0.8950000000000007\n",
      "score: 0.8912, lambda:0.9000000000000007\n",
      "score: 0.8967, lambda:0.9050000000000007\n",
      "score: 0.8937, lambda:0.9100000000000007\n",
      "score: 0.8898, lambda:0.9150000000000007\n",
      "score: 0.8983, lambda:0.9200000000000007\n",
      "score: 0.8914, lambda:0.9250000000000007\n",
      "score: 0.8961, lambda:0.9300000000000007\n",
      "score: 0.8967, lambda:0.9350000000000007\n",
      "score: 0.8928, lambda:0.9400000000000007\n",
      "score: 0.8943, lambda:0.9450000000000007\n",
      "score: 0.8979, lambda:0.9500000000000007\n",
      "score: 0.8933, lambda:0.9550000000000007\n",
      "score: 0.8914, lambda:0.9600000000000007\n",
      "score: 0.8931, lambda:0.9650000000000007\n",
      "score: 0.8926, lambda:0.9700000000000008\n",
      "score: 0.8974, lambda:0.9750000000000008\n",
      "score: 0.8916, lambda:0.9800000000000008\n",
      "score: 0.8933, lambda:0.9850000000000008\n",
      "score: 0.8916, lambda:0.9900000000000008\n",
      "score: 0.8926, lambda:0.9950000000000008\n",
      "max score: 0.9, lambda: 0.09500000000000001\n"
     ]
    }
   ],
   "source": [
    "X = np.hstack((np.ones((np.size(X_val, 0),1)),X_val))\n",
    "T = y_val\n",
    "X_ = np.hstack((np.ones((np.size(X_test, 0),1)),X_test))\n",
    "T_ = y_test\n",
    "K = np.size(T, 1)\n",
    "M = np.size(X, 1)\n",
    "W = np.zeros((M,K))\n",
    "\n",
    "iterations = 10000\n",
    "learning_rate = 0.01\n",
    "\n",
    "initial_cost = compute_cost(X, T, W, 0)\n",
    "\n",
    "lambda_ = 0\n",
    "max_score = 0\n",
    "max_lambda = 0\n",
    "while(lambda_ <= 1):\n",
    "    (cost_history, W_optimal) = batch_gd(X, T, W, learning_rate, iterations, lambda_, 64)\n",
    "    y_pred = predict(X_, W_optimal)\n",
    "    score = float(sum(y_pred == np.argmax(T_, axis=1)))/ float(len(y_test))\n",
    "    print(\"score: {}, lambda:{}\".format(score, lambda_))\n",
    "    if max_score < score:\n",
    "        max_score = score\n",
    "        max_lambda = lambda_\n",
    "    lambda_ += 0.005\n",
    "print(\"max score: {}, lambda: {}\".format(max_score, max_lambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones((np.size(X_train, 0),1)),X_train))\n",
    "T = y_train\n",
    "\n",
    "K = np.size(T, 1)\n",
    "M = np.size(X, 1)\n",
    "W = np.zeros((M,K))\n",
    "\n",
    "iterations = 50000\n",
    "learning_rate = 0.01\n",
    "\n",
    "initial_cost = compute_cost(X, T, W, max_lambda)\n",
    "\n",
    "(cost_history, W_optimal) = batch_gd(X, T, W, learning_rate, iterations, max_lambda,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91\n"
     ]
    }
   ],
   "source": [
    "## Accuracy\n",
    "X_ = np.hstack((np.ones((np.size(X_test, 0),1)),X_test))\n",
    "T_ = y_test\n",
    "y_pred = predict(X_, W_optimal)\n",
    "score = float(sum(y_pred == np.argmax(T_, axis=1)))/ float(len(y_test))\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
